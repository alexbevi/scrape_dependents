## Introduction

Identifying popular dependents (repositories that depend on a project and have many stars) helps prioritize compatibility, discover high-impact users, and surface integration opportunities. This tool extracts dependents and ranks them by stars so you can focus on the most influential dependents first.

## Quick summary

- Script: `src/scrape_dependents.mjs`
- Per-repo output: `output/reports/*.md`
- Aggregated index: `output/README.md` (generated separately)

## Requirements

- Node.js (modern runtime; Node 18+ recommended)
- Internet access to GitHub

## Install

```bash
# from project root
npm install
```

## Quick usage

Run the scraper for a single repository (owner/name):

```bash
npm run scrape -- --repo=OWNER/NAME
```

Common options:

- `--repo` (string, required) — source repository in `owner/name` form
- `--min-stars` (number, default 0) — drop dependents with fewer stars than this
- `--max-pages` (number, default 0) — max dependents pages to crawl (0 = unlimited)
- `--include-forks` (boolean) — include forked repos
- `--sleep-ms` (number, default 150) — delay between page fetches (ms)
- `--output-dir` (string, default `output`) — where reports and README are written
- `--package_id` (string, optional) — if set, appended to dependents query (used for language-specific package views)

# dependents-scraper

Small Node.js tool to crawl GitHub repository dependents pages and emit per-repo Markdown reports.

Summary
 - Script: `src/scrape_dependents.mjs`
 - Per-repo output: `output/reports/*.md`
 - Aggregated index: `output/README.md` (generated separately)

Requirements
 - Node.js 18+ recommended
 - Internet access to GitHub

Install

```bash
# from project root
npm install
```

Quick usage

Run the scraper for a single repository:

```bash
npm run scrape -- --repo=OWNER/NAME
```

Important CLI options
- `--repo` (required): source repository `owner/name`
- `--min-stars` (default 0): filter dependents with fewer stars
- `--max-pages` (default 0 = unlimited)
# dependents-scraper

Dependents-scraper crawls GitHub "dependents" pages and produces per-repository reports so you can identify the most impactful projects depending on your codebase. A key goal is surfacing high-star dependents (popular users of a project) so maintainers can prioritize compatibility and outreach.

What this repo contains
- `src/scrape_dependents.mjs` — main scraper (ESM). Fetches dependents pages, parses repositories, extracts stars/forks, and writes a per-repo Markdown report to `output/reports/`.
- `src/generate_readme.mjs` — builds `output/README.md` from the files found in `output/reports/` (grouped by `type` and sorted).
- Workflows:
	- `.github/workflows/dependents.yml` — reusable workflow run per-repo (matrix item). It runs the scraper for a single repo and commits the generated report file.
	- `.github/workflows/scrape_dependents.yml` — top-level dispatcher that builds the matrix from `repos.json` and invokes the reusable workflow for each repo; it calls `finalize_readme` after the matrix completes.
	- `.github/workflows/finalize_readme.yml` — reusable (and dispatchable) workflow that runs `src/generate_readme.mjs` and commits or opens a PR for the aggregated `output/README.md`.

Quick start

1. Install dependencies:

```bash
npm ci
```

2. Run the scraper locally for a single repo:

```bash
npm run scrape -- --repo=OWNER/NAME
```

3. Per-repo reports will be written to `output/reports/OWNER-NAME-dependents.md`.

Command-line options (high level)
- `--repo` (required) — `owner/name` of the repo to scrape
- `--min-stars` — minimum stargazers to include (default `0`)
- `--max-pages` — max dependents pages to crawl (default `0` = unlimited)
- `--sleep-ms` — ms delay between page fetches (default `150`)
- `--output-dir` — where reports are stored (default `output`)
- `--package_id` — optional query param used by some GitHub pages to scope dependents

Output format
- Per-repo Markdown report at `output/reports/<owner>-<name>-dependents.md` with a table of dependents and a summary block (pages scraped, repos found, filtered count, total possible from GitHub when available, and percent processed).
- Aggregated `output/README.md` is generated by `src/generate_readme.mjs` and groups reports by `type`.

CI behavior and safety
- Matrix runs: each matrix job calls the reusable `dependents.yml`. That job writes the per-repo report and attempts to commit it to the repository. The commit step:
	- fetches origin and rebases to avoid non-fast-forward pushes
	- stashes local untracked/generated files before pulling
	- force-adds `output` (to capture generated files even if `.gitignore` ignores them)
	- commits and pushes the generated report file
	- if rebase or stash/restore fails, the job will skip pushing to avoid corrupting `main` (and may create a PR in some configurations)

- Final README: after the matrix, the dispatcher calls `finalize_readme.yml` once. That job runs `src/generate_readme.mjs` and attempts a rebase-first push; if push fails it creates a branch and opens a PR with the aggregated README so maintainers can review/merge.

Why this approach
- Per-repo commits avoid collisions: each job updates a distinct `output/reports/<repo>.md` file so concurrent pushes rarely conflict.
- The aggregated README is generated once after the matrix to avoid race conditions and inconsistent tables.

Running only the finalize step
- You can generate and push the aggregated README without re-running scrapes by dispatching the `finalize_readme` workflow:

	- In GitHub UI: Actions → Finalize README → Run workflow
	- Or with `gh`:

```bash
gh workflow run finalize_readme.yml --repo <owner>/<repo>
```